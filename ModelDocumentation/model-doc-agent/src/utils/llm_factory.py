from langchain_anthropic import ChatAnthropic
from typing import Optional, Any, Dict, List
import os
from .config import Config
from langchain_community.llms.fake import FakeListLLM
from langchain.schema import Generation
from langchain.prompts import ChatPromptTemplate

# Initialize a global config instance or pass it as an argument
# For simplicity here, we assume a global or accessible config instance
# In a larger app, dependency injection or a context manager might be better.
_config = Config() 

def create_llm(
    model_name: Optional[str] = None,
    temperature: Optional[float] = None, 
    max_tokens: Optional[int] = None,
    api_key: Optional[str] = None,
    debugger: Optional[Any] = None,
    mock_mode: bool = False,
    mock_responses: Optional[List[str]] = None
):
    """
    Create a consistent LLM instance using global config or overrides.
    
    Args:
        model_name: Model name to use (defaults to config value)
        temperature: Temperature setting (defaults to config value)
        max_tokens: Maximum tokens to generate (defaults to config value)
        api_key: Optional explicit API key
        debugger: Optional debugger instance for logging
        mock_mode: If True, returns a mock LLM for testing without API
        mock_responses: Optional list of responses for the mock LLM
    
    Returns:
        A configured LLM instance (real or mock)
    """
    
    # Log if debugger is provided
    if debugger:
        debugger.logger.debug(f"Creating LLM with model: {model_name or _config.get('llm.model', 'claude-3-opus-20240229')}")
    
    # Use mock mode if specified or if no API key is available in test/dev
    use_mock = mock_mode
    
    # Check if API key exists
    anthropic_api_key = api_key or _config.get("anthropic_api_key") or os.getenv("ANTHROPIC_API_KEY")
    
    # If no API key and not explicitly requesting mock_mode, check environment
    if not anthropic_api_key and not mock_mode:
        # Use mock in development if no API key and no explicit request for real
        if _config.get("environment", "development") == "development":
            use_mock = True
            if debugger:
                debugger.logger.warning("No API key found, switching to mock LLM mode")
        # In production, we might want to raise an error instead
        elif _config.get("environment") == "production":
            raise ValueError("Anthropic API key is required in production environment")
    
    if use_mock:
        # Create a mock LLM for testing without API access
        default_responses = mock_responses or [
            "This is a mock response for testing purposes. In a real environment, this would be generated by Claude.",
            "Here's another mock response. Your Model Documentation Agent is running in mock mode.",
            "I've analyzed the code and created a summary based on the provided information."
        ]
        
        if debugger:
            debugger.logger.warning("Using FakeListLLM for testing without API key")
        
        return FakeListLLM(
            responses=default_responses
        )
    
    # Get config values with defaults    
    model_to_use = model_name if model_name is not None else _config.get("llm.model", "claude-3-opus-20240229")
    temp_to_use = temperature if temperature is not None else _config.get("llm.temperature", 0.2)
    tokens_to_use = max_tokens if max_tokens is not None else _config.get("llm.max_tokens", 4096)
    
    # Initialize ChatAnthropic with the specified parameters
    llm = ChatAnthropic(
        model=model_to_use,
        temperature=temp_to_use,
        max_tokens_to_sample=tokens_to_use,
        anthropic_api_key=anthropic_api_key
    )
    
    if debugger:
        debugger.logger.debug(f"LLM initialized with temperature: {temp_to_use}, max_tokens: {tokens_to_use}")
    
    return llm 